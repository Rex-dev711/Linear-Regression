{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Given below are the theory, mathematical concepts and code examples of Simple Linear Regression model having a single feature using the Gradient Descent Algorithm!!\n",
        "\n",
        "A majority of the theory and notation is used from Professor Andrew Ng's lectures!!"
      ],
      "metadata": {
        "id": "ciudU49VppLb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Simple Linear Regression with One Feature** **(Univariate Linear Regression)**\n",
        "\n",
        "**Overview:** Simple linear regression is a statistical method used to model the relationship between a single independent variable (feature) and a dependent variable (target/label). The goal is to find a linear equation that best predicts the target variable based on the feature. The 'hypothesis' of a simple linear regression model can be expressed as:\n",
        "\\begin{equation}\n",
        "h(x) = \\theta_0 + \\theta_1 x\n",
        "\\end{equation}\n",
        "\n",
        "where:\n",
        "\n",
        "h(x) is the hypothesis/model\n",
        "\n",
        "x is the independent variable\n",
        "\n",
        "$\\theta_0$ is the y-intercept or the bias term\n",
        "\n",
        "$\\theta_1$ is the slope of the line (coefficient of the feature)\n",
        "\n",
        "$\\theta_0$ & $\\theta_1$ together are also called the parameters or coefficients or the weights\n",
        "\n",
        "Note the above hypothesis is an affine function\n"
      ],
      "metadata": {
        "id": "RkLupi73qQel"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Cost/Loss Function: Mean Squared Error (MSE) cost function**\n",
        "\n",
        "The most common cost function used in linear regression is the Mean Squared Error (MSE), which measures the average squared difference between the predicted values and the actual values:\n",
        "\n",
        "\\begin{equation}\n",
        "J(\\theta_0, \\theta_1) = \\frac{1}{2m} \\sum_{i=1}^{m} (h_\\theta(x^{(i)}) - y^{(i)})^2\n",
        "\\end{equation}\n",
        "\n",
        "where:\n",
        "m is the number of training examples\n",
        "\n",
        "$h_\\theta(x^{(i)})$ is the hypothesis function, which predicts output for *i*-th training example\n",
        "\n",
        "$y^{(i)}$ is the i-th training example\n",
        "\n",
        "$\\theta_0$ & $\\theta_1$ are the parameters of the linear regression model\n",
        "\n",
        "The mean squared cost function measures the average squared error between the predicted output and the actual output for all training examples.\n",
        "\n",
        "The goal of the linear regression is to find the values of $\\theta_0$ & $\\theta_1$ that minimizes the cost function J($\\theta_0$, $\\theta_1$)\n",
        "\n",
        "The graph of the cost function vs $\\theta_1$ vs $\\theta_0$ is a 3-Dimensional curve with a bowl shape that may have more than one global minimum. But since we're using squared error, we can only have a single global minimum.\n",
        "\n",
        "You can also use a countor plot to visualize it better"
      ],
      "metadata": {
        "id": "OqqbCigOxJ1Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Gradient Descent Algorithm**\n",
        "\n",
        "\n",
        "The outline of the algorithm is to start with a random value of $\\theta_0$ & $\\theta_1$ and keep changing both to reduce J($\\theta_0$, $\\theta_1$) until we settle at or near a minima\n",
        "\n",
        "The program looks like this:\n",
        "\n",
        "repeat until convergence{\n",
        "  \\begin{equation}\n",
        "    \\theta_0 = \\theta_0 - \\alpha \\frac{\\partial}{\\partial \\theta_0} J(\\theta_0, \\theta_1)\n",
        "  \\end{equation}\n",
        "\n",
        "  \\begin{equation}\n",
        "    \\theta_1 = \\theta_1 - \\alpha \\frac{\\partial}{\\partial \\theta_1} J (\\theta_0, \\theta_1)\n",
        "  \\end{equation}\n",
        "}\n",
        "\n",
        "where $\\alpha$ is the \"learning rate\" that determines how big of a step to take for each iteration\n",
        "\n",
        "Here you have to simultaneously update $\\theta_0$ & $\\theta_1$ and take the pre-updated value in the partial derivative\n",
        "\n",
        "The derivative shows you the slope(+/-) & it helps to get closer to the minimum, that's the reason there's the partial derivative term in the equation\n",
        "\n",
        "The equation then becomes:\n",
        "\n",
        "repeat until convergence{\n",
        "  \\begin{equation}\n",
        "    \\theta_0 = \\theta_0 - \\alpha (\\frac{1}{m} \\sum_{i=1}^{m} (h(\\theta_0 ^ {(i)}) - y^{(i)}))\n",
        "  \\end{equation}\n",
        "\n",
        "  \\begin{equation}\n",
        "    \\theta_1 = \\theta_1 - \\alpha (\\frac{1}{m} \\sum_{i=1}^{m} (h(\\theta_1 ^ {(i)}) - y^{(i)})x^{(i)})\n",
        "  \\end{equation}\n",
        "}"
      ],
      "metadata": {
        "id": "hJnHfj8b3Vii"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "That's probably all there is to for an overview of SLR & Gradient Descent algorithm. Now to apply it \"manually\" using only the Numpy library"
      ],
      "metadata": {
        "id": "LutHf5SR8xWU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Import all the necessary libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "XWMRUmvQ9DqE"
      },
      "execution_count": 129,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Generate some synthetic data\n",
        "X = np.random.randn(100, 1) #generate 100 random integers\n",
        "Y = 2 * X + np.random.randn(100, 1) #linear relation with some noise"
      ],
      "metadata": {
        "id": "8we8yG-b9Lt2"
      },
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Implementing Gradient Descent\n",
        "\n",
        "#setting up the hyperparameters\n",
        "learning_rate = 0.01\n",
        "iterations = 1000\n",
        "m = X.shape[0] #number of training examples\n",
        "\n",
        "#initializing the parameters\n",
        "theta_0 = 0\n",
        "theta_1 = 0\n",
        "\n",
        "#save the cost for plotting later\n",
        "Cost = []\n",
        "\n",
        "# gradient descent\n",
        "for iteration in range(iterations):\n",
        "    h = theta_0 + theta_1 * X\n",
        "    gradientTheta_0 = (1/m) * np.sum(h - Y)\n",
        "    gradientTheta_1 = (1/m) * np.sum((h - Y) * X)\n",
        "    theta_0 -= learning_rate * gradientTheta_0\n",
        "    theta_1 -= learning_rate * gradientTheta_1\n",
        "    cost = (1/(2*m)) * np.sum((h - Y) ** 2)\n",
        "    Cost.append(cost)\n",
        "\n",
        "# Plotting the cost over iterations\n",
        "plt.plot(range(iterations), Cost)\n",
        "plt.xlabel('Iterations')\n",
        "plt.ylabel('Cost')\n",
        "plt.title('Cost vs Iterations')\n",
        "plt.show()\n",
        "\n",
        "# Print final parameters\n",
        "print(f'Final parameters: theta_0 = {theta_0}, theta_1 = {theta_1}')\n",
        "\n",
        "# Predicting the regression line\n",
        "Y_pred = theta_0 + theta_1 * X\n",
        "\n",
        "# Plotting the results\n",
        "plt.scatter(X, Y, label='Actual Data')\n",
        "plt.plot(X, Y_pred, color='red', label='Regression Line')\n",
        "plt.xlabel('X')\n",
        "plt.ylabel('Y')\n",
        "plt.title('Simple Linear Regression')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "IzHue5AI9vCW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The above code converges right around at 200 iterations for learning rate equal to 0.01, however you can choose different values of $\\alpha$, $\\theta_0$ & $\\theta_1$ and see when the algorithm converges and what's the linear equation!!"
      ],
      "metadata": {
        "id": "MKUzcoSuLXZl"
      }
    }
  ]
}
