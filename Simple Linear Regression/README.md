### **__Simple Linear Regression__**

---

This repository contains a collection of Simple Linear Regression algorithms, including their theoretical explanations and code implementations. The purpose of this repository is to provide a comprehensive overview of different linear regression methods, demonstrate how they work, and offer working examples to help users understand their practical applications.

## ðŸ“šTable of Contents
1. [Introduction](#introduction)
2. [Algorithms Included](#algorithms-included)
3. [Guide](#guide)

## Introduction
This folder explores various algorithms used for simple linear regression, from traditional methods like Ordinary Least Squares (OLS) to regularized versions like Ridge and Lasso.

The algorithms are implemented in Python and are designed for educational purposes to help you understand the mathematical principles behind simple linear regression and how they are applied in practice.

## Algorithms Included
The following regression algorithms are included in this folder:

1.  Linear Least Squares Method: The classic linear regression method based on minimizing the sum of squared residuals.

2.  Gradient Descent Algorithm: An iterative optimization technique used to minimize the cost function.

3.  Normal Equation Method: A closed-form solution that directly calculates the model parameters.

4.  Newton-Raphson Method: An iterative optimization technique that uses both the first and second derivatives of the cost function to find the optimal parameters.

## Guide
A simple guide to follow the code examples and theories are:

1. [Linear Least Squares Method](https://github.com/Rex-dev711/Linear-Regression/blob/main/Simple%20Linear%20Regression/SLR_leastSquares.ipynb)

2. [Gradient Descent](https://github.com/Rex-dev711/Linear-Regression/blob/main/Simple%20Linear%20Regression/SLR_gradientDescent.ipynb)

3. [Normal Equation](https://github.com/Rex-dev711/Linear-Regression/blob/main/Simple%20Linear%20Regression/SLR_normalEquation.ipynb)
