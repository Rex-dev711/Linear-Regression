{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Simple Linear Regression using Newton-Raphson Method**\n",
        "___"
      ],
      "metadata": {
        "id": "-oQXMP4wvU0V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Newton-Raphson method is a root-finding algorithm that can be used to solve optimization problems, including those involved in simple linear regression. In the context of linear regression, the method can be used to minimize the cost function. We know that the hypothesis and the cost functions are:\n",
        "\n",
        "\\begin{equation}\n",
        "h_{\\theta}(x) = \\theta_0 + \\theta_1 x\n",
        "\\end{equation}\n",
        "\n",
        "\\begin{equation}\n",
        "J(\\theta) = \\frac{1}{2m} \\sum_{i = 1}^{m} (h_{\\theta}x_{i} - y_{i})^2\n",
        "\\end{equation}\n",
        "\n",
        "where y_i is the true value!!"
      ],
      "metadata": {
        "id": "YtMVDhMavhsp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To apply the Newton-Raphson method, we need\n",
        "\n",
        "First derivative (Gradient): The gradient of the cost function $\\nabla J(\\theta)$ which tells us the direction of steepest ascent, and\n",
        "Second derivative (Hessian): The Hessian matrix, which is the matrix of second derivatives $H(\\theta) = \\nabla^2 J(\\theta)$ gives us information about the curvature of the cost function.\n",
        "\n",
        "Without going into the calculus and derivations, the formula takes a generalized form as:\n",
        "\n",
        "\\begin{equation}\n",
        "\\theta^{(k+1)} = \\theta^{(k)} - H(\\theta^{(k)})^{-1} \\nabla J(\\theta^{(k)})\n",
        "\\end{equation}\n",
        "\n",
        "where k is the current iteration\n",
        "\n",
        "This formula updates the parameter vector $\\vec{\\theta}$ in the direction opposite to the gradient, scaled by the inverse of the Hessian matrix. The inverse Hessian ensures that the updates are adjusted for the curvature of the objective function, providing a more efficient convergence than basic gradient descent.\n",
        "\n",
        "\n",
        "Now let's apply the method:"
      ],
      "metadata": {
        "id": "R-0gjVgfx_ZY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Importing necessary libraries and generating some random synthetic data + defining some terms\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "m = 100 #number of data points\n",
        "x = np.random.rand(m) * 10  # Random x values between 0 and 10\n",
        "\n",
        "#you can choose the below 2 values urself\n",
        "theta_0_true, theta_1_true = 2.0, 3.5  # True intercept & slope\n",
        "error = np.random.randn(m)  # Random noise\n",
        "\n",
        "#The true function\n",
        "y = theta_0_true + theta_1_true * x + error"
      ],
      "metadata": {
        "id": "tVa0lIKw1NQq"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now defining the cost/objective function, gradient and Hessian"
      ],
      "metadata": {
        "id": "29qgAThY2kZQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining the cost function\n",
        "def cost_function(theta, x, y):\n",
        "  theta_0, theta_1 = theta\n",
        "  return np.sum((y - (theta_0 + theta_1 * x))**2)\n",
        "\n",
        "#computing the gradient (first derivative of cost function)\n",
        "def gradient(theta, x, y):\n",
        "  theta_0, theta_1 = theta\n",
        "  grad_theta_0 = -2 * np.sum(y - (theta_0 + theta_1 * x))\n",
        "  grad_theta_1 = -2 * np.sum((y - (theta_0 + theta_1 * x)) * x)\n",
        "  return np.array([grad_theta_0, grad_theta_1])\n",
        "  #if you're wondering where this came from, this is just the derivative of cost fxn w.r.t to \\theta_0 & \\theta_1\n",
        "\n",
        "def hessian(theta, x):\n",
        "  theta_0, theta_1 = theta\n",
        "  hess_00 = 2 * len(x)  # Second derivative w.r.t. theta_0\n",
        "  hess_01 = 2 * np.sum(x)  # Mixed second derivative\n",
        "  hess_11 = 2 * np.sum(x**2)  # Second derivative w.r.t. theta_1\n",
        "  return np.array([[hess_00, hess_01], [hess_01, hess_11]])"
      ],
      "metadata": {
        "id": "vXaiRHtw2hrG"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initializing the parameters & starting the method"
      ],
      "metadata": {
        "id": "ta-wytJw6OeV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "theta_init = np.array([0.0, 0.0])  # Initial guess for theta_0 and theta_1\n",
        "tol = 1e-6  # Convergence tolerance\n",
        "max_iter = 100  # Maximum number of iterations\n",
        "theta = theta_init  # Starting parameters\n",
        "\n",
        "#Newton-Raphson Method\n",
        "for i in range(max_iter):\n",
        "    grad = gradient(theta, x, y)\n",
        "    H = hessian(theta, x)\n",
        "    H_inv = np.linalg.inv(H)  # Inverse of the Hessian matrix\n",
        "    theta_new = theta - H_inv @ grad  # Newton-Raphson update\n",
        "\n",
        "    # Check for convergence (change in parameters is small)\n",
        "    if np.linalg.norm(theta_new - theta) < tol:\n",
        "        print(f\"Converged after {i+1} iterations\")\n",
        "        break\n",
        "    theta = theta_new"
      ],
      "metadata": {
        "id": "MvTItY7J6YqP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The predicted parameters and the results are:"
      ],
      "metadata": {
        "id": "gOzqR0PG6xhJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Estimated parameters\n",
        "theta_0_est, theta_1_est = theta\n",
        "print(f\"Estimated intercept (theta_0): {theta_0_est:.3f}\")\n",
        "print(f\"Estimated slope (theta_1): {theta_1_est:.3f}\")\n",
        "\n",
        "#Plotting the results\n",
        "plt.scatter(x, y, label=\"Noisy data\", color=\"blue\")\n",
        "plt.plot(x, theta_0_true + theta_1_true * x, label=\"True line\", color=\"red\")\n",
        "plt.plot(x, theta_0_est + theta_1_est * x, label=\"Fitted line (Newton-Raphson)\", color=\"green\")\n",
        "plt.xlabel(\"x\")\n",
        "plt.ylabel(\"y\")\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.title(\"Linear Regression using Newton-Raphson\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "83RspBoj62gY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You might notice that the above code can be solved faster and without iteration with the help of the normal equation method we talked before. But still this is to show an example of how to use the Newton-Raphson method. This method can converge faster (as the cost function is quadratic and convex) but is computationally expensive as it requires to calculate the second order Hessian matrix, which can increase with the increase of the number of features. But in case of simple linear regression, it doesn't cost much."
      ],
      "metadata": {
        "id": "aVrBiHh64Wb7"
      }
    }
  ]
}