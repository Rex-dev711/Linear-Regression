{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Simple Linear Regression using Normal Equation**\n",
        "\n",
        "The Normal Equation is another method for finding the parameters (or coefficients) of a linear regression model. The goal is to minimize the cost function (typically Mean Squared Error, or MSE) in order to find the best-fit line.\n",
        "\n",
        "In case of SLR, the hypothesis is:\n",
        "\n",
        "\\begin{equation}\n",
        "h(x) = \\theta_0 + \\theta_1 x\n",
        "\\end{equation}\n",
        "\n",
        "where:\n",
        "\n",
        "h(x) is the hypothesis/model\n",
        "\n",
        "x is the independent variable\n",
        "\n",
        "$\\theta_0$ is the y-intercept or the bias term\n",
        "\n",
        "$\\theta_1$ is the slope of the line (coefficient of the feature)\n",
        "\n",
        "$\\theta_0$ & $\\theta_1$ together are also called the parameters or coefficients or the weights\n",
        "\n",
        "Note the above hypothesis is an affine function"
      ],
      "metadata": {
        "id": "KFUI_dS_0yyK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implementing the normal equation:\n",
        "\n",
        "The normal equation for linear regression is derived from the least squares optimization, which minimizes the error between the predicted values and the actual values. The equation is:\n",
        "\n",
        "\\begin{equation}\n",
        "\\theta = (X^T X)^{-1} X^T h(x)\n",
        "\\end{equation}\n",
        "\n",
        "where:\n",
        "\n",
        "\n",
        "X is the matrix of input features (with an extra column of ones added to account for the intercept)\n",
        "\n",
        "$X^T$ is the transpose of X\n",
        "\n",
        "$\\theta$ is the vector for the parameters of the model which gives the global minimum of the cost function J($\\theta$),\n",
        "\n",
        "\\begin{equation}\n",
        "J(\\theta) = \\frac{1}{2m} \\sum_{i = 1}^{m} (h_{\\theta}(x^{(i)}) - y^{(i)})^2\n",
        "\\end{equation}\n",
        "\n",
        "\n",
        "The soultion of the below equation gives the global minimum,\n",
        "\n",
        "\\begin{equation}\n",
        "\\nabla_{\\theta} J(\\theta) = \\vec{0}\n",
        "\\end{equation}"
      ],
      "metadata": {
        "id": "Uj5dfEa92Hyn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "While the gradient descent algorithm iteratively update the parameters by moving in the direction of the negative gradient of the cost function, the 'normal equation' method provides an analytical solution to the global minimum of the cost function, given that $(X^T X)^{-1}$ exists, in a single step.\n",
        "\n",
        "While the normal equation is faster and more effective, the computation can become expensive when there are multiple features (X is large) and it doesn't work if $(X^T X)^{-1}$ isn't invertible."
      ],
      "metadata": {
        "id": "xuTl8_JV80Qg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's get to the code. First, import the necessary libraries and generate the data"
      ],
      "metadata": {
        "id": "jxRYy_FC_dof"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Importing necessary libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "qQh3hqik_3sE"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Generating data\n",
        "m = 100  # Number of data points\n",
        "X = 5 * np.random.rand(m, 1)  # Random values from 0 to 5 for feature X\n",
        "y = 4 + 3 * X + np.random.randn(m, 1)  # Linear relation with noise: y = 4 + 3*X + noise\n",
        "\n",
        "#Add a column of ones to represent the intercept\n",
        "X_b = np.c_[np.ones((m, 1)), X] #X_b is of shape (m, 2)"
      ],
      "metadata": {
        "id": "nS7OnX7M_-Zm"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, implementing the normal equation in one line of code:"
      ],
      "metadata": {
        "id": "NiWS2n0_Asc4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Normal Equation\n",
        "theta = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y)"
      ],
      "metadata": {
        "id": "k6qh3WH-Aqx5"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The parameters are then:"
      ],
      "metadata": {
        "id": "NFaVGi5pBJjb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "theta_0, theta_1 = theta[0], theta[1]\n",
        "print(f\"The parameter theta_0: {theta_0}\")\n",
        "print(f\"The parameter theta_1: {theta_1}\")\n",
        "\n",
        "#Ploting the result\n",
        "plt.scatter(X, y, color='blue', label='Data points')  # Plot the data points\n",
        "plt.plot(np.array([[0], [5]]), np.c_[np.ones((2, 1)), X_new].dot(theta), color='red', label='Best-fit line')  # Plot the best-fit line\n",
        "plt.xlabel('X')\n",
        "plt.ylabel('y')\n",
        "plt.title('Linear Regression using Normal Equation')\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "SyKnnBoiBJM5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}